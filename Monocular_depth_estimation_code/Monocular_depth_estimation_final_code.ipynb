{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf1\n",
    "from scipy import ndimage\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building blocks of ED-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input_images, weight, stride = 1):\n",
    "    return tf.nn.conv2d(input_images, weight ,strides=[1,stride, stride,1], padding='VALID',name=\"conv\")\n",
    "        \n",
    "\n",
    "def conv_elu(input_layer, k, in_filter, ou_filter, stride, scope, activation = tf.nn.elu, reuse=False):\n",
    "    \n",
    "    with tf.compat.v1.variable_scope(scope, reuse = reuse):\n",
    "        W = tf.compat.v1.get_variable(\"weight\", [k, k, in_filter, ou_filter], initializer=tf1.glorot_uniform_initializer())\n",
    "        b = tf.compat.v1.get_variable(\"biases\", [ou_filter],initializer=tf1.glorot_uniform_initializer())\n",
    "        \n",
    "        #we need pyrimad which output is half of its input so, need to pad input.\n",
    "        p = np.floor((k - 1) / 2).astype('int32')\n",
    "        padding = tf.constant([[0,0],[p, p],[p, p],[0,0]])\n",
    "        p_x = tf.pad(input_layer, padding)\n",
    "        \n",
    "        #padded input\n",
    "        conv = conv2d(p_x, W, stride = stride)\n",
    "        output = tf.nn.bias_add(conv, b)\n",
    "        out = activation(output)\n",
    "        \n",
    "    return out\n",
    "\n",
    "\n",
    "def upsampling(input_layer, factor):    \n",
    "    return tf.keras.layers.UpSampling2D(size=(factor, factor))(input_layer)\n",
    "    \n",
    "             \n",
    "\n",
    "def upconv(input_layer, k, in_filter, ou_filter, scope, reuse=False):\n",
    "    \n",
    "    with tf.compat.v1.variable_scope(scope, reuse = reuse):    \n",
    "        #Upsampling\n",
    "        upsample = upsampling(input_layer, 2)\n",
    "        out = conv_elu(upsample, k, in_filter, ou_filter, 1, scope='conv_elu')\n",
    "        return out\n",
    "\n",
    "\n",
    "def conv_block(input_layer, k, in_filter, ou_filter, scope):\n",
    "    \n",
    "    c1 = conv_elu(input_layer, k,  in_filter, ou_filter,  1, scope=scope )\n",
    "    c2 = conv_elu(c1,          k,  ou_filter, ou_filter,  2, scope=scope+'b')\n",
    "    \n",
    "    return c2\n",
    "    \n",
    "def get_disp(x, in_filter,scope):\n",
    "    disp = 0.3 * conv_elu(x, 3, in_filter, 2, 1, scope = scope, activation = tf.nn.sigmoid)\n",
    "    return disp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture of ED-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_placeholder():\n",
    "    \n",
    "    tf1.disable_eager_execution()\n",
    "    tf1.reset_default_graph()\n",
    "    \n",
    "    with tf1.name_scope(\"Input_image\"):\n",
    "        input_layer = tf1.placeholder('float', shape = input_shape)\n",
    "        \n",
    "    return input_layer\n",
    "\n",
    "\n",
    "def make_architecture(input_layers):\n",
    "    \n",
    "    with tf1.name_scope(\"ENCNN\"):\n",
    "    \n",
    "        with tf1.name_scope(\"encoder\"):\n",
    "            conv1 = conv_block(input_layers, 7,  3,  32, 'conv1')#2\n",
    "            conv2 = conv_block(conv1,        5, 32,  64, 'conv2')#4\n",
    "            conv3 = conv_block(conv2,        3, 64, 128, 'conv3')#8\n",
    "            conv4 = conv_block(conv3,        3, 128,256, 'conv4')#16\n",
    "            conv5 = conv_block(conv4,        3, 256,512, 'conv5')#32\n",
    "            conv6 = conv_block(conv5,        3, 512,512, 'conv6')#64\n",
    "            conv7 = conv_block(conv6,        3, 512,512, 'conv7')#128\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        with tf1.name_scope(\"decoder\"):\n",
    "            #upsampling 7\n",
    "            upconv7 = upconv(conv7,     3,  512,  512, scope =  'upconv7')\n",
    "            concat7 = tf.concat([upconv7, conv6], axis=-1, name = 'concat7')\n",
    "            iconv7  = conv_elu(concat7, 3, 1024,  512, 1, scope= 'iconv7')\n",
    "\n",
    "            #upsampling 6\n",
    "            upconv6 = upconv(iconv7,    3,  512,  512, scope =  'upconv6')\n",
    "            concat6 = tf.concat([upconv6, conv5], axis=-1, name = 'concat6')\n",
    "            iconv6  = conv_elu(concat6, 3, 1024,  512, 1, scope= 'iconv6')\n",
    "\n",
    "            #upsampling 5\n",
    "            upconv5 = upconv(iconv6,    3,  512,  256, scope =  'upconv5')\n",
    "            concat5 = tf.concat([upconv5, conv4], axis=-1, name= 'concat5')\n",
    "            iconv5  = conv_elu(concat5, 3,  512,  256, 1, scope= 'iconv5')\n",
    "\n",
    "            #upsampling 4\n",
    "            upconv4 = upconv(iconv5,    3,   256,  128, scope = 'upconv4')\n",
    "            concat4 = tf.concat([upconv4, conv3], axis=-1, name ='concat4')\n",
    "            iconv4  = conv_elu(concat4, 3, 256,  128, 1, scope= 'iconv4')\n",
    "            disp4   = get_disp(iconv4, 128, scope= 'disp4')\n",
    "            updisp4 = upsampling(disp4, 2)\n",
    "\n",
    "            #upsampling 3\n",
    "            upconv3 = upconv(iconv4,    3,  128,  64, scope = 'upconv3')\n",
    "            concat3 = tf.concat([upconv3, conv2, updisp4], axis=-1, name='concat3')\n",
    "            iconv3  = conv_elu(concat3, 3, 130,  64, 1, scope= 'iconv3')\n",
    "            disp3   = get_disp(iconv3,  64, scope = 'disp3')\n",
    "            updisp3 = upsampling(disp3, 2)\n",
    "\n",
    "            #upsampling 2\n",
    "            upconv2 = upconv(iconv3,    3,  64,   32, scope = 'upconv2')\n",
    "            concat2 = tf.concat([upconv2, conv1, updisp3], axis=-1, name='concat2')\n",
    "            iconv2  = conv_elu(concat2, 3,  66,   32, 1, scope= 'iconv2')\n",
    "            disp2   = get_disp(iconv2,  32, scope = 'disp2')\n",
    "            updisp2 = upsampling(disp2, 2)\n",
    "\n",
    "            #upsampling 1\n",
    "            upconv1 = upconv(iconv2,    3,  32,   16, scope = 'upconv1')\n",
    "            concat1 = tf.concat([upconv1, updisp2], axis=-1, name='convat1')\n",
    "            iconv1  = conv_elu(concat1, 3,  18,   16, 1, scope= 'iconv1')\n",
    "            disp1   = get_disp(iconv1,  16, scope = 'disp1')\n",
    "        \n",
    "    return disp1, disp2, disp3 ,disp4\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_input_image):\n",
    "    logdir = '/media/sansii/Software/san_projects/Major_project/Moncular_depth_estimation_data/'\n",
    "    input_layer = init_placeholder()\n",
    "    \n",
    "    disp1, disp2, disp3 , disp4 = make_architecture(input_layer)\n",
    "    \n",
    "    writer = tf1.summary.FileWriter(logdir+'./graph' , graph=tf1.get_default_graph())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSIM(image, pred_image, block_size):\n",
    "    \n",
    "    C1 = 0.01 ** 2\n",
    "    C2 = 0.03 ** 2\n",
    "    \n",
    "    u_x = tf.nn.avg_pool2d(image      ,block_size, strides=1, padding='SAME')\n",
    "    u_y = tf.nn.avg_pool2d(pred_image ,block_size, strides=1, padding='SAME')\n",
    "\n",
    "    sigma_x  = tf.nn.avg_pool2d(image**2           ,block_size, strides=1, padding='SAME') - u_x**2\n",
    "    sigma_y  = tf.nn.avg_pool2d(pred_image**2      ,block_size, strides=1, padding='SAME') - u_y**2\n",
    "    sigma_xy = tf.nn.avg_pool2d(image * pred_image ,block_size, strides=1, padding='SAME') - u_x * u_y\n",
    "    \n",
    "    SSIM_num = ((2 * u_x * u_y + C1)   * (2 * sigma_x * sigma_y   + C2)) \n",
    "    SSIM_den = ((u_x**2 + u_y**2 + C1) * (sigma_x**2 + sigma_y**2 + C2))\n",
    "    \n",
    "    SSIM = SSIM_num / SSIM_den\n",
    "    \n",
    "    return tf.clip_by_value((1 - SSIM) / 2, 0, 1)\n",
    "\n",
    "def apperance_matching_loss(image, pred_image):\n",
    "    alpha = 0.85\n",
    "    L1_error = tf.abs(image - pred_image)\n",
    "    ssim_error = SSIM(image, pred_image, 3)\n",
    "    \n",
    "    C_ap = tf.reduce_mean((alpha * ssim_error + (1 - alpha) * L1_error))\n",
    "    \n",
    "    return apperance_matching_loss\n",
    "\n",
    "\n",
    "def disparity_smoothness_loss(disp, image):\n",
    "    \n",
    "    disp_gradient_y , disp_gradient_x  = tf.image.image_gradients(disp)\n",
    "    image_gradient_y, image_gradient_x = tf.image.image_gradients(image)\n",
    "\n",
    "    im_dx = -tf.reduce_mean(tf.abs(image_gradient_x),axis=-1, keepdims=True)\n",
    "    im_dy = -tf.reduce_mean(tf.abs(image_gradient_y),axis=-1, keepdims=True)\n",
    "    \n",
    "    loss_dx = tf.multiply(tf.abs(disp_gradient_x), tf.math.exp(im_dx))\n",
    "    loss_dy = tf.multiply(tf.abs(disp_gradient_y), tf.math.exp(im_dy))\n",
    "    \n",
    "    disp_smoothness_loss = tf.reduce_mean((loss_dx + loss_dy))\n",
    "    \n",
    "    \n",
    "    return disp_smoothness_loss\n",
    "\n",
    "\n",
    "def left_right_consistency_loss(disp_left, disp_right):\n",
    "    pass\n",
    "     \n",
    "        \n",
    "\n",
    "\n",
    "def training_loss(image, pred_image):\n",
    "    #Apperance Loss\n",
    "    alpha = 0.85\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Transformer Network (Bilinear Sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STN:\n",
    "    def interpolate(self, img, x, y):\n",
    "        #bilinear_interpolation\n",
    "\n",
    "        #For corner pixel there is no either left or right / top or down pixels so padding is necessary\n",
    "        img = tf.pad(img, paddings= ((0,0),(1,1),(1,1),(0,0)))\n",
    "        \n",
    "        x = tf.clip_by_value(x, 0.0, tf.cast(self.width,tf.float32)+1.0)\n",
    "\n",
    "        #since we have padded we need to add plus 1 for our transformed coordinates\n",
    "        x = x + 1\n",
    "        y = y + 1\n",
    "\n",
    "        #since the values are in fraction we need to take floor value which selects left pixels\n",
    "        x_float   = tf.floor(x)\n",
    "        y_float   = tf.floor(y)\n",
    "        x_1_float = x_float + 1\n",
    "        \n",
    "        x_1_float = tf.minimum(x_1_float, tf.cast(self.width,tf.float32)+1.0 )\n",
    "        \n",
    "        #Since,the index are in integer we convert float into integer\n",
    "        x_int = tf.cast(x_float, tf.int32)\n",
    "        y_int = tf.cast(y_float, tf.int32)\n",
    "        x_1_int = tf.cast(x_1_float, tf.int32)\n",
    "        \n",
    "        #we required total dimention for reshaping \n",
    "        dim_y  = self.width + 2 * 1#padding\n",
    "        dim_xy = (self.width + 2) * (self.height + 2)\n",
    "\n",
    "        #There are number of images with there individual coordinate space now we need to convert\n",
    "        #individual coordinate space into a single coordinate space\n",
    "        #eg: x = [ [0,1,2,3], [0,1,2,3], [0,1,2,3]] \n",
    "        # y= [[0,0,0,0],[1,1,1,1],[2,2,2,2]]into [0 ,1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "        #Converting 2d spatial dimention into 1d vector\n",
    "        \n",
    "        base = tf.tile(tf.expand_dims(tf.range(self.num_batches),1) * dim_xy, [1, self.width * self.height])\n",
    "        base = tf.reshape(base, [-1])\n",
    "    \n",
    "        x_l = x_int   + (base + y_int * dim_y)\n",
    "        x_r = x_1_int + (base + y_int * dim_y)\n",
    "        \n",
    "        print(x_l.numpy())\n",
    "        #Flattering input image\n",
    "        img_flat = tf.reshape(img, [-1, self.num_channels]) \n",
    "        \n",
    "        #tf.gather selects pixels of img from coordinate x_l and x_r\n",
    "        #Therefore pixel_l and pixel_r contains only selected coordinates pixels from img\n",
    "        pixel_l = tf.gather(img_flat, x_l)\n",
    "        pixel_r = tf.gather(img_flat, x_r)\n",
    "        \n",
    "        #Now for bilinear interpolation each left and right pixel must be associated with its respective weights.\n",
    "        weights_l = tf.expand_dims(x - x_float, 1)\n",
    "        weights_r = tf.expand_dims(x_1_float - x, 1)\n",
    "        \n",
    "        print(\"Weight\", weights_l.shape)\n",
    "        print(\"pixel_l\", pixel_l.shape)\n",
    "        \n",
    "        out = (weights_l * pixel_l) + (weights_r * pixel_r)\n",
    "\n",
    "        return tf.reshape(out, [self.num_batches, self.height, self.width, self.num_channels])\n",
    "\n",
    "    def bilinear_sampling(self, img, disparity):\n",
    "        self.img = img\n",
    "        self.width       = tf.shape(img)[2]\n",
    "        self.height      = tf.shape(img)[1]\n",
    "        self.num_batches = tf.shape(img)[0]\n",
    "        self.num_channels= tf.shape(img)[3]\n",
    "\n",
    "        width_f =  tf.cast(self.width , tf.float32)\n",
    "        height_f = tf.cast(self.height, tf.float32)\n",
    "\n",
    "        #Creating meshgrid which contains represents coordinates(x, y) of images\n",
    "        x_grid, y_grid = tf.cast(tf.meshgrid(tf.range(self.width), \n",
    "                                     tf.range(self.height)), tf.float32)\n",
    "\n",
    "        #Flatterning grids\n",
    "        x_flat = tf.reshape(x_grid, [-1])\n",
    "        y_flat = tf.reshape(y_grid, [-1])\n",
    "\n",
    "        #Since, there are num_batches so we need to add up grids for all batches\n",
    "        x_flat = tf.tile(x_flat, [self.num_batches])\n",
    "        y_flat = tf.tile(y_flat, [self.num_batches])\n",
    "\n",
    "        #Flatterning disparity size = (num_batches * weight * height)\n",
    "        disparity_flat = tf.reshape(disparity, [-1])\n",
    "\n",
    "        #Adding disparity to find / applying transformation\n",
    "        x_transf = x_flat + (disparity_flat * width_f) #Since the out of sigmoid funtion in 0 -1 so we muliply with width\n",
    "\n",
    "        #Transformed coordinates are in fraction since, there is no fraction pixel so we interplate\n",
    "        out = self.interpolate(img, x_transf, y_flat)\n",
    "\n",
    "\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
